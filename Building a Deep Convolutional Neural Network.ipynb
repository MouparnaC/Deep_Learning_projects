{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7xaZOY4gfjR"
   },
   "source": [
    "**Building a \"Deep\" Convolutional Neural Network**\n",
    "\n",
    "Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8e-mdtkoQhvB"
   },
   "outputs": [],
   "source": [
    "# importing necessary tools\n",
    "%matplotlib inline\n",
    "import numpy as np                      #advanced math library\n",
    "import matplotlib.pyplot as plt         #MATLAB like plotting routine\n",
    "import random                           #For generating random numbers\n",
    "# importing some additional tools also\n",
    "from keras.datasets import mnist        # MNIST dataset is included in Keras    \n",
    "from keras.models import Sequential     # Model type to be used\n",
    "\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation  # Types of layers to import\n",
    "from keras.utils import np_utils                          # NumPy related tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf54dlOshaFq"
   },
   "source": [
    "Image generator\n",
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PdAlxCCwREkY"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D,GlobalAveragePooling2D,Flatten\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocecqibMS_uY",
    "outputId": "a2b00179-7905-453c-d108-59e064cce225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "X_train_shape (60000, 28, 28)\n",
      "X_test_shape (10000, 28, 28)\n",
      "y_train_shape (60000,)\n",
      "y_test_shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "#The MNIST data is split between 60,000 images (28 X 28 pixel)\n",
    "#training images and 10,000 (28 X 28 pixel) images\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "\n",
    "print(\"X_train_shape\", X_train.shape)\n",
    "print(\"X_test_shape\", X_test.shape)\n",
    "print(\"y_train_shape\", y_train.shape)\n",
    "print(\"y_test_shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uD-TY-EAUltH"
   },
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(60000,28,28,1)         # reshape 60,000 28 X 28 matrices into\n",
    "X_test=X_test.reshape(10000,28,28,1)           # reshape 10,000 28 X 28 matrices into\n",
    "\n",
    "X_train=X_train.astype('float32')              #change integers to 32-bit floating point\n",
    "X_test=X_test.astype('float32')\n",
    "\n",
    "X_train /=255                                  #normalize each value for each pixel\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KnI0lG1zUzPZ"
   },
   "outputs": [],
   "source": [
    "# one hot format classes\n",
    "nb_classes=10    # number of unique digits\n",
    "\n",
    "Y_train=np_utils.to_categorical(y_train,nb_classes)\n",
    "Y_test=np_utils.to_categorical(y_test,nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2Fg4kpilU6A6"
   },
   "outputs": [],
   "source": [
    "from keras.engine import sequential\n",
    "# The Sequential model is a linear stack of layers and is very common\n",
    "model= Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i_TgtMY1U9YY"
   },
   "outputs": [],
   "source": [
    "#Convolution Layer 1\n",
    "model.add(Conv2D(32,(3,3),input_shape=(28,28,1)))  #32 different 3X3 Kernels, so 32 feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2sesLbck76z"
   },
   "source": [
    "When we compute a BatchNormalization along an axis, we preserve the dimensions of the array, and we normalize with respect to the mean and standard deviation over every other axis. So in your 2D example BatchNormalization with axis=1 is subtracting the mean for axis=0, just as you expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K-ry_cpXVTT8"
   },
   "outputs": [],
   "source": [
    "model.add(BatchNormalization(axis=-1))       # normalize each feature map before activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "85FIsr7oVZ9r"
   },
   "outputs": [],
   "source": [
    "convLayer01 = Activation('relu')             # activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TNJqJZ0FVg6u"
   },
   "outputs": [],
   "source": [
    "#ConvolutionLayer2\n",
    "model.add(Conv2D(32,(3,3)))                  #32 different 3X3 Kernels, so 32 feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dfDe8rYJVvWs"
   },
   "outputs": [],
   "source": [
    "model.add(BatchNormalization(axis=-1))       # normalize each feature map before activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oNcTAkc1mXEv"
   },
   "outputs": [],
   "source": [
    "convLayer02 = Activation('relu')             # activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mAf8Ob_GVzs3"
   },
   "outputs": [],
   "source": [
    "convLayer02=MaxPooling2D(pool_size=(2,2))    # Pool the max values over a 2X2 Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LEz4LSZOV8Vp"
   },
   "outputs": [],
   "source": [
    "model.add(convLayer02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1M8Vi7D0WDyj"
   },
   "outputs": [],
   "source": [
    "#convolutionLayer3\n",
    "model.add(Conv2D(64,(3,3)))                     # 64 different 3X3 Kernels, so 64 feature maps\n",
    "model.add(BatchNormalization(axis=-1))          # normalize each feature map before activation\n",
    "convLayer03=Activation('relu')                  # activation\n",
    "model.add(convLayer03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Us4ss2qLWhze"
   },
   "outputs": [],
   "source": [
    "#convolutionLayer4\n",
    "model.add(Conv2D(64,(3,3)))                     # 64 different 3X3 Kernels, so 64 feature maps\n",
    "model.add(BatchNormalization(axis=-1))          # normalize each feature map before activation\n",
    "convLayer03=Activation('relu')                  # activation\n",
    "convLayer04=MaxPooling2D(pool_size=(2,2))       # Pool the max values over a 2X2 Kernel\n",
    "model.add(convLayer04)\n",
    "model.add(Flatten())                            # flatten final 4X4X64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "B6Cg_A_AW7Rm"
   },
   "outputs": [],
   "source": [
    "#Fully Connected Layer 5\n",
    "model.add(Dense(512))                           # 512 FCN nodes\n",
    "model.add(BatchNormalization())                 # normalisation\n",
    "model.add(Activation('relu'))                   # activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1tKMVXCQXPn3"
   },
   "outputs": [],
   "source": [
    "#Fully Connected layer 6\n",
    "model.add(Dropout(0.2))                         # 20% dropout of random \n",
    "model.add(Dense(10))                            # final 10 FCN nodes\n",
    "model.add(Activation('softmax'))                # softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEuPmsceXuJm",
    "outputId": "1690a6ef-d184-41ce-e591-7fb839e31f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10, 10, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10, 10, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 597,738\n",
      "Trainable params: 596,330\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Summarize the built model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "McxxfJybXwGc"
   },
   "outputs": [],
   "source": [
    "# let's use the Adam optimizer for learning \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pipkSoA9YE9w"
   },
   "outputs": [],
   "source": [
    "# data augmentation prevents overfitting by slightly changing the data randomly\n",
    "# Keras has a great built-in feature to do automatic augmentation\n",
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08,\n",
    "                         shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xqnbAjiiYLAn"
   },
   "outputs": [],
   "source": [
    "# We can then feed our augmented data in batches\n",
    "# Besides loss function consideration as before, this method actually results in significant memory savings because we are actually\n",
    "# LOADING the data into the network in batches before processing each batch\n",
    "\n",
    "train_generator= gen.flow(X_train,Y_train,batch_size=128)\n",
    "test_generator= test_gen.flow(X_test,Y_test,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QppYIREbY2It",
    "outputId": "c26abc5a-e1f6-4689-9195-de50bc6be2b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "468/468 [==============================] - 206s 437ms/step - loss: 0.1598 - accuracy: 0.9505 - val_loss: 0.1303 - val_accuracy: 0.9615\n",
      "Epoch 2/5\n",
      "468/468 [==============================] - 194s 414ms/step - loss: 0.0589 - accuracy: 0.9819 - val_loss: 0.0296 - val_accuracy: 0.9911\n",
      "Epoch 3/5\n",
      "468/468 [==============================] - 190s 405ms/step - loss: 0.0460 - accuracy: 0.9856 - val_loss: 0.0422 - val_accuracy: 0.9868\n",
      "Epoch 4/5\n",
      "468/468 [==============================] - 190s 406ms/step - loss: 0.0399 - accuracy: 0.9878 - val_loss: 0.0282 - val_accuracy: 0.9915\n",
      "Epoch 5/5\n",
      "468/468 [==============================] - 190s 406ms/step - loss: 0.0373 - accuracy: 0.9880 - val_loss: 0.0312 - val_accuracy: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fa363b650>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now train our model which is fed data by our batch loader steps per epoch should always be total\n",
    "# size of the set divided by the batch size\n",
    "\n",
    "# SIGNIFICANT MEMORY SAVINGS (important for larger, deeper networks)\n",
    "model.fit_generator(train_generator,steps_per_epoch=60000//128,epochs=5,verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=10000//128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwp1Q1OgbUp7",
    "outputId": "ec507287-ad6e-4043-f31a-492ccd8e617f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 27ms/step - loss: 0.0314 - accuracy: 0.9905\n",
      "Test score: 0.0313723124563694\n",
      "Test accuracy: 0.9904999732971191\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(X_test,Y_test)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6rXTdU2ejXPx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
